{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning - Discrete State\n",
    "\n",
    "This notebook contains the codes for reinforcement learning in a world with discrete states and discrete actions. It draws out the Gridworld, and implements policy evaluation, policy iteration and value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gridworld and functions for policy evaluation, policy iteration and value iteration\n",
    "\n",
    "class Gridworld(object):\n",
    "    def __init__(self):\n",
    "        ############### Attributes of the Gridworld ################\n",
    "        #shape\n",
    "        self.shape = (6,6)\n",
    "        #obstacle locations\n",
    "        self.obstacle = [(1,1),(2,3),(2,5),(3,1),(4,1),(4,2),(4,4)]\n",
    "        #terminal states\n",
    "        self.terminal_locs = [(1,3),(4,3)]\n",
    "        #terminal rewards\n",
    "        self.term_reward = [10, -100]\n",
    "        #rewards for other states\n",
    "        self.other_reward = -1\n",
    "        #starting location for value_iteration\n",
    "        self.start = (2,0)\n",
    "        #actions\n",
    "        self.action = ['N', 'E', 'S', 'W']\n",
    "        #number of actions\n",
    "        self.action_size = len(self.action)\n",
    "        #randoming action results\n",
    "        self.action_randomising = [0.55, 0.15, 0.15, 0.15]\n",
    "        ###########################################################\n",
    "        \n",
    "        ############## Internal states ############################\n",
    "        \n",
    "        #get attributes of the world\n",
    "        state_size, T, R, terminal, locs, state_neighbours = self.build_gridworld()\n",
    "        #to find who the neighbours are\n",
    "        self.state_neighbours = state_neighbours\n",
    "        #number of valid states\n",
    "        self.state_size = state_size\n",
    "        #transition matrix\n",
    "        self.T = T\n",
    "        #reward matrix \n",
    "        self.R = R\n",
    "        #terminal states\n",
    "        self.terminal = terminal\n",
    "        #the locations of the valid states\n",
    "        self.locs = locs\n",
    "        #the locations of the valid states excluding terminal states\n",
    "        self.move_locs = self.get_movable_locs(self.locs)\n",
    "        #the number of the starting state for random start\n",
    "        self.starting_state = self.loc_to_state(self.start, locs);\n",
    "        #locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0, self.starting_state] = 1\n",
    "        \n",
    "        #placing walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle:\n",
    "            self.walls[ob]=1\n",
    "        \n",
    "        #placing terminals on a grid for illustration\n",
    "        self.terminals = np.zeros(self.shape)\n",
    "        for ter in self.terminal_locs:\n",
    "            self.terminals[ter] = -1\n",
    "        \n",
    "        #placing the reward on a grid for illustration\n",
    "        self.rewards = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.terminal_locs):\n",
    "            self.rewards[rew] = self.term_reward[i]\n",
    "        \n",
    "        #illustrating the gridworld\n",
    "        self.paint_maps()\n",
    "        \n",
    "        #############################################################\n",
    "\n",
    "        \n",
    "\n",
    "    ################## Internal functions #########################\n",
    "    def get_movable_locs(self, locs):   \n",
    "        #return the locations that are valid and not terminal\n",
    "        \n",
    "        #remove the terminal states\n",
    "        move_loca = locs.copy()\n",
    "        for loc in move_loca:\n",
    "            if loc == self.terminal_locs[0]:\n",
    "                move_loca.remove(loc)\n",
    "            elif loc == self.terminal_locs[1]:\n",
    "                move_loca.remove(loc)\n",
    "        \n",
    "        #get all the valid and none terminal locations\n",
    "        move_locs = []\n",
    "        for loca in move_loca:\n",
    "            loci = self.loc_to_state(loca, locs)\n",
    "            move_locs.append(loci)\n",
    "            \n",
    "        return move_locs\n",
    "                \n",
    "    \n",
    "    def build_gridworld(self):\n",
    "        #get the locations of all valid states and their neighbours by state number\n",
    "        #and the terminal states (array of 0's with ones in the terminal state)\n",
    "        \n",
    "        locations, neighbours, terminal = self.get_topology()\n",
    "        #get the number of states\n",
    "        S = len(locations)\n",
    "        #initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        #randomise the outcome of taking an action\n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                outcome=(action+effect+1)%4\n",
    "                if outcome == 0:\n",
    "                    outcome ==3\n",
    "                else:\n",
    "                    outcome == -1\n",
    "                \n",
    "                #fill the transition matrix\n",
    "                prob = self.action_randomising[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state, prior_state,action]=T[post_state, prior_state,action]+prob\n",
    "                \n",
    "        #build the reward matrix\n",
    "        R = self.other_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.term_reward):\n",
    "            post_state = self.loc_to_state(self.terminal_locs[i],locations)\n",
    "            R[post_state,:,:] = sr\n",
    "        \n",
    "        return S, T, R, terminal, locations, neighbours\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_topology(self):\n",
    "        #obtain the locations and neighbours of every state in the Gridworld\n",
    "        width = self.shape[1]\n",
    "        height = self.shape[0]\n",
    "        \n",
    "        index = 1\n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                #get the location of each state\n",
    "                loc = (i,j)\n",
    "                #append to the valid state locs if it is in the grid and not an obstacle\n",
    "                if (self.is_loc(loc)):\n",
    "                    locs.append(loc)\n",
    "                    #get an array of neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc, direction) for direction in ['nr', 'ea', 'so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        #translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                #find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                #turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc, locs)\n",
    "                #insert into neighbour matrix\n",
    "                state_neighbours[state][direction]=nstate;\n",
    "        \n",
    "        #translates terminal locations into terminal state indices\n",
    "        terminal = np.zeros((1, num_states))\n",
    "        for a in self.terminal_locs:\n",
    "            terminal_state = self.loc_to_state(a, locs)\n",
    "            terminal[0][terminal_state]=1\n",
    "        \n",
    "        return locs, state_neighbours, terminal\n",
    "\n",
    "    \n",
    "    def is_loc(self, loc):\n",
    "        #location is valid if it is in the grid and not an obstacle\n",
    "        if (loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif (loc in self.obstacle):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #find the valid neighbours i.e. in the grid and not obstacles\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        #if the neighbour is a valid location, accept it, otherwise, don't move\n",
    "        if (direction == 'nr' and self.is_loc(nr)):\n",
    "            return nr\n",
    "        elif (direction == 'ea' and self.is_loc(ea)):\n",
    "            return ea\n",
    "        elif (direction == 'so' and self.is_loc(so)):\n",
    "            return so\n",
    "        elif (direction == 'we' and self.is_loc(we)):\n",
    "            return we\n",
    "        else:\n",
    "            return loc\n",
    "            \n",
    "    \n",
    "    def loc_to_state(self, loc, locs):\n",
    "        #takes list of locations and gives the index of the corresponding location\n",
    "        return locs.index(tuple(loc))\n",
    "    \n",
    "    \n",
    "    def paint_maps(self):\n",
    "        #draw the Gridworld\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.title('obstacles')\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.terminals)\n",
    "        plt.title('terminal states')\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewards)\n",
    "        plt.title('reward states')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    def epsilon_greedy_action(self, policy, initial):\n",
    "        #choose an action given an epsilon-greedy policy\n",
    "        \n",
    "        all_moves = policy[int(initial),:]\n",
    "        a = all_moves[0]\n",
    "        b = a + all_moves[1]\n",
    "        c= b + all_moves[2]\n",
    "        d = c + all_moves[3]\n",
    "        roll1 = np.random.rand()\n",
    "        if roll1<=a:\n",
    "            mov = 0\n",
    "        elif roll1 >a and roll1<=b:\n",
    "            mov = 1\n",
    "        elif roll1 >b and roll1<=c:\n",
    "            mov = 2\n",
    "        elif roll1 >c and roll1 <=d:\n",
    "            mov = 3\n",
    "        return mov\n",
    "\n",
    "    \n",
    "    def get_trace(self, policy):\n",
    "        #policy should be a 29x4 matrix\n",
    "        \n",
    "        #get the reward matrix, which is not known to agent but needed for getting trace\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        #store the state, action and reward of each step in three separate lists\n",
    "        S = []\n",
    "        A = []\n",
    "        Re = []\n",
    "        \n",
    "        start = np.random.randint(len(self.move_locs))\n",
    "        S.append(start)\n",
    "        \n",
    "        #find the terminal state numbers\n",
    "        terminating = []\n",
    "        for a in self.terminal_locs:\n",
    "            terminal_state = self.loc_to_state(a, self.locs)\n",
    "            terminating.append(terminal_state)\n",
    "        \n",
    "        #keep going until reaching the terminal state\n",
    "        while S[-1] != terminating[0] and S[-1] != terminating[1]:\n",
    "            initial = S[-1]\n",
    "            \n",
    "            #generate move in nr, ea, so or we according to the policy\n",
    "            move = self.epsilon_greedy_action(policy, initial)\n",
    "           \n",
    "            #generate the actual move after choosing the action\n",
    "            w = self.action_randomising[0]\n",
    "            x= w + self.action_randomising[1]\n",
    "            y= x + self.action_randomising[2]\n",
    "            z = y +self.action_randomising[3]\n",
    "            roll2 = np.random.rand()\n",
    "            if roll2<=w:\n",
    "                action = move\n",
    "            elif roll2>w and roll2<=x:\n",
    "                action = (move+1)%4\n",
    "            elif roll2>x and roll2<=y:\n",
    "                action = (move+2)%4\n",
    "            elif roll2>y and roll2<=z:\n",
    "                action = (move+3)%4\n",
    "            \n",
    "            #get the final state\n",
    "            final = self.state_neighbours[int(initial)][action]\n",
    "            rewa = R[int(final), int(initial), int(action)]\n",
    "            A.append(move)\n",
    "            Re.append(rewa)\n",
    "            S.append(final)\n",
    "        \n",
    "        return S,A,Re\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##################################################################################\n",
    "\n",
    "          \n",
    "    ############# Get info ###########\n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    #################################\n",
    "    \n",
    "\n",
    "            \n",
    "    ################################# Methods #######################################\n",
    "    def td_learning(self, iteration, alpha, epsilon, gamma):\n",
    "        #get the reward matrix\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        #initialise the state-action values and a policy\n",
    "        Q = np.zeros((self.state_size, self.action_size))\n",
    "        policy = np.zeros((self.state_size, self.action_size))\n",
    "        \n",
    "        #set arbitrary policy\n",
    "        policy[:,0]=((1-epsilon)+(epsilon/self.action_size))\n",
    "        policy[:,1:4] = epsilon/self.action_size\n",
    "        \n",
    "        #find the terminal state numbers\n",
    "        terminating = []\n",
    "        for a in self.terminal_locs:\n",
    "            terminal_state = self.loc_to_state(a, self.locs)\n",
    "            terminating.append(terminal_state)\n",
    "        \n",
    "        #get the total reward\n",
    "        total_reward = []\n",
    "        \n",
    "        #get the state values for all states after each iteration, stored as a nested list\n",
    "        intervalues = []\n",
    "        \n",
    "        #start learning\n",
    "        for i in range(iteration):\n",
    "            start = np.random.randint(len(self.move_locs))\n",
    "            S = start\n",
    "            #generate an action in nr, ea, so or we according to the eps-greedy policy\n",
    "            A = self.epsilon_greedy_action(policy, S)\n",
    "            rew = 0\n",
    "            #take the action, observe the reward and S'\n",
    "            count = -1\n",
    "            while S != terminating[0] and S != terminating[1]:\n",
    "                \n",
    "                count += 1\n",
    "                \n",
    "                #generate the actual move after choosing the action and the next state\n",
    "                w = self.action_randomising[0]\n",
    "                x= w+self.action_randomising[1]\n",
    "                y= x+ self.action_randomising[2]\n",
    "                z = y +self.action_randomising[3]\n",
    "                roll2 = np.random.rand()\n",
    "                if roll2<=w:\n",
    "                    action = A\n",
    "                elif roll2>w and roll2<=x:\n",
    "                    action = (A+1)%4\n",
    "                elif roll2>x and roll2<=y:\n",
    "                    action = (A+2)%4\n",
    "                elif roll2>y and roll2<=z:\n",
    "                    action = (A+3)%4\n",
    "            \n",
    "                #get S' and R\n",
    "                S_prime = self.state_neighbours[int(S)][action]\n",
    "                immed_reward = R[int(S_prime), int(S), int(action)]\n",
    "                rew+= immed_reward*(gamma**count)\n",
    "                \n",
    "                #get A'\n",
    "                A_prime = self.epsilon_greedy_action(policy, S_prime)\n",
    "                Q[int(S),int(A)] = Q[int(S),int(A)]+alpha*(immed_reward+gamma*Q[int(S_prime),int(A_prime)]-Q[int(S),int(A)])\n",
    "                policy[int(S)] = epsilon/self.action_size\n",
    "                policy[int(S)][np.argmax(Q[int(S),:])] = ((1-epsilon)+(epsilon/self.action_size))\n",
    "                S = S_prime\n",
    "                A = A_prime\n",
    "            \n",
    "            #get the current value functions\n",
    "            intervalue = np.zeros(Q.shape[0])\n",
    "            for i in range(Q.shape[0]):\n",
    "                intervalue[i] = np.max(Q[i,:])\n",
    "            intervalues.append(intervalue)\n",
    "    \n",
    "            total_reward.append(rew)\n",
    "        \n",
    "        return Q, policy, total_reward, intervalues\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def monte_carlo(self, iteration, alpha, epsilon, gamma):\n",
    "        \n",
    "        #initiate Q and policy\n",
    "        Q = np.zeros((self.state_size, self.action_size))\n",
    "        policy = np.zeros((self.state_size, self.action_size))\n",
    "        \n",
    "        #set arbitrary policy\n",
    "        policy[:,0]=((1-epsilon)+(epsilon/self.action_size))\n",
    "        policy[:,1:4] = epsilon/self.action_size\n",
    "        \n",
    "        total_reward = []\n",
    "        intervalues = []\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            #get trace with the policy\n",
    "            S, A, Re = self.get_trace(policy)\n",
    "            #drop the terminal state, for ease of analysis\n",
    "            S.pop(-1)\n",
    "            \n",
    "            #store the already-visited states in this list,\n",
    "            #so they are not counted twice in one trace\n",
    "            gone = []\n",
    "            \n",
    "            #get reward for this episode\n",
    "            total = 0\n",
    "            for kk in range(len(Re)):\n",
    "                total += Re[kk]*(gamma**kk)\n",
    "            total_reward.append(total)\n",
    "            \n",
    "            for j in range(len(S)):\n",
    "                if not (S[j]*100+A[j] in gone):\n",
    "                    haha = S[j]*100+A[j]\n",
    "                    gone.append(haha)\n",
    "                    ret = 0\n",
    "                    #calculate total discounted return from the occurence of S,A\n",
    "                    for lol in range(len(S)-j):\n",
    "                        ret += (gamma**lol)*Re[lol+j]\n",
    "                    #update state-action value\n",
    "                    Q[int(S[j]),int(A[j])] = Q[int(S[j]),int(A[j])] + alpha*(ret - Q[int(S[j]),int(A[j])])\n",
    "            \n",
    "            #get the current value functions\n",
    "            intervalue = np.zeros(Q.shape[0])\n",
    "            for i in range(Q.shape[0]):\n",
    "                intervalue[i] = np.max(Q[i,:])\n",
    "            intervalues.append(intervalue)\n",
    "            \n",
    "            #set policy to be epsilon greedy\n",
    "            for state in range(self.state_size):\n",
    "                if not state in self.terminal:\n",
    "                    policy[state] = epsilon/self.action_size\n",
    "                    policy[state][np.argmax(Q[state,:])] = ((1-epsilon)+(epsilon/self.action_size))\n",
    "        \n",
    "        return Q, policy, total_reward, intervalues\n",
    "\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, gamma, threshold):\n",
    "        #get transition and reward matrices\n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        #rate is the number of iterations\n",
    "        rate = 0\n",
    "        delta = threshold\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        #keep going until delta falls below the threshold\n",
    "        while delta >= threshold:\n",
    "            rate += 1\n",
    "            delta = 0\n",
    "            \n",
    "            for st in range(self.state_size):\n",
    "                if not (self.terminal[0, st]):\n",
    "                    v = V[st]\n",
    "                    #compute the Q value\n",
    "                    Q = np.zeros(4)\n",
    "                    for stp in range(self.state_size):\n",
    "                        Q+= T[stp, st,:]*(R[stp, st, :]+gamma*V[stp])\n",
    "                    \n",
    "                    V[st]=np.max(Q)\n",
    "                    \n",
    "                    delta = max(delta, np.abs(v - V[st]))\n",
    "        \n",
    "        #when the while loop is finished\n",
    "        opt_policy = np.zeros((self.state_size, self.action_size))\n",
    "        \n",
    "        for st in range(self.state_size):\n",
    "            #compute Q value\n",
    "            Q = np.zeros(4)\n",
    "            for stp in range(self.state_size):\n",
    "                Q += T[stp,st,:] * (R[stp,st, :] + gamma * V[stp])\n",
    "            \n",
    "            #set the optimal policy to be the action with the highest return in each state\n",
    "            opt_policy[st, np.argmax(Q)]=1\n",
    "        \n",
    "        return V, opt_policy, rate\n",
    "                \n",
    "    \n",
    "\n",
    "    def draw_deterministic_policy(self, policy):\n",
    "        #draw a deterministic policy\n",
    "        #the policy needs to be a np array of 29 values between 0 and 3 with\n",
    "        # N->0, E->1, S->2, W->3\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewards+self.terminals)\n",
    "        for state, action in enumerate(policy):\n",
    "            if(self.terminal[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1],location[0],action_arrow,ha='center', va='center')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def draw_value(self, Value):\n",
    "        # Draw a policy value function\n",
    "        # The value need to be a np array of 29 values \n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewards +self.terminals) # Create the graph of the grid\n",
    "        for state, value in enumerate(Value):\n",
    "            if(self.terminal[0,state]): # If it is an absorbing state, don't plot any value\n",
    "                continue\n",
    "            location = self.locs[state] # Compute the value location on graph\n",
    "            plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning\n",
    "\n",
    "grid = Gridworld()\n",
    "\n",
    "values, policy, iteration = grid.value_iteration(0.4, 0.00001)\n",
    "\n",
    "\n",
    "optimal_Q, opt_policy, total_re, intvalues = grid.monte_carlo(2000, 0.03, 0.6, 0.4)\n",
    "n=optimal_Q.shape[0]\n",
    "\n",
    "tdQ, tdpolicy, tdreward, tdvalues = grid.td_learning(2000, 0.03, 0.6, 0.4)\n",
    "\n",
    "RMS = []\n",
    "for i in range(len(intvalues)):\n",
    "    error = 0\n",
    "    for j in range(n):\n",
    "        error += (intvalues[i][j]-values[j])**2\n",
    "    haha = (error/n)**(0.5)\n",
    "    RMS.append(haha)\n",
    "\n",
    "tdRMS = []\n",
    "for i in range(len(tdvalues)):\n",
    "    error = 0\n",
    "    for j in range(n):\n",
    "        error += (tdvalues[i][j]-values[j])**2\n",
    "    haha = (error/n)**(0.5)\n",
    "    tdRMS.append(haha)\n",
    "    \n",
    "    \n",
    "plt.figure()\n",
    "plt.scatter(tdRMS, tdreward, s=15)\n",
    "plt.xlabel(\"Root mean square error\")\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, policy, iteration = grid.policy_iteration(0.4, 0.0001)\n",
    "\n",
    "grid.draw_deterministic_policy(np.array([np.argmax(policy[row,:]) for row in range(grid.state_size)]))\n",
    "grid.draw_value(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate an arbitrary policy using draw_deterministic_policy()\n",
    "\n",
    "Policy2 = np.zeros(29).astype(int)\n",
    "Policy2[2] = 3\n",
    "Policy2[6] = 2\n",
    "Policy2[18] = 1\n",
    "grid.draw_deterministic_policy(Policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros((grid.state_size, grid.action_size))\n",
    "policy = policy + 0.25\n",
    "print(\"The policy is : {}\".format(policy))\n",
    "\n",
    "val, iterations = grid.policy_evaluation(policy, 0.01, 0.4)\n",
    "print(\"The value of that policy is : {}\".format(val))\n",
    "print(\"Convergence took place after {} iterations.\".format(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How learning rate affects optimisation\n",
    "\n",
    "grid = Gridworld()\n",
    "\n",
    "alpha = [0.05, 0.06, 0.8]\n",
    "\n",
    "alphass = []\n",
    "for alp in alpha:\n",
    "    alphh = []\n",
    "    for i in range(200):\n",
    "        optimal_Q, opt_policy, total_re = grid.td_learning(500, alp, 0.6, 0.4)\n",
    "        alphh.append(total_re)\n",
    "    alphass.append(alphh)\n",
    "    \n",
    "plt.figure()\n",
    "for i in range(len(alphass)):\n",
    "    haha = alphass[i]\n",
    "    rewa00 = np.array(haha)\n",
    "    rewa00.reshape(200,500)\n",
    "    rewa0 = np.mean(rewa00, axis=0)\n",
    "    plt.plot(range(500), rewa0)\n",
    "plt.xlabel(\"number of episodes, alpha varies\")\n",
    "plt.ylabel(\"total reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the exploration parameter affects optimisation\n",
    "\n",
    "grid = Gridworld()\n",
    "\n",
    "epsilon = [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "epss = []\n",
    "for eps in epsilon:\n",
    "    epp = []\n",
    "    for i in range(1000):\n",
    "        optimal_Q, opt_policy, total_re = grid.td_learning(100, 0.03, eps, 0.4)\n",
    "        epp.append(total_re)\n",
    "    epss.append(epp)\n",
    "    \n",
    "plt.figure()\n",
    "for i in range(len(epss)):\n",
    "    eps0 = epss[i]\n",
    "    rewa00 = np.array(eps0)\n",
    "    rewa00.reshape(1000,100)\n",
    "    rewa0 = np.mean(rewa00, axis=0)\n",
    "    plt.plot(range(100), rewa0)\n",
    "plt.xlabel(\"number of episodes, epsilon varies\")\n",
    "plt.ylabel(\"total reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards\n",
    "grid = Gridworld()\n",
    "\n",
    "\n",
    "rewardss = []\n",
    "for i in range(2000):\n",
    "    optimal_Q, opt_policy, total_re = grid.td_learning(100, 0.03, 0.6, 0.4)\n",
    "    rewardss.append(total_re)\n",
    "\n",
    "rewa = np.array(rewardss)\n",
    "rewa.reshape(2000,100)\n",
    "avg_re = np.mean(rewa, axis=0)\n",
    "re_std = np.std(rewa, axis=0)\n",
    "avg_less = avg_re - re_std\n",
    "avg_more = avg_re + re_std\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(100), avg_re)\n",
    "plt.plot(range(100), avg_less)\n",
    "plt.plot(range(100), avg_more)\n",
    "plt.xlabel(\"number of episodes\")\n",
    "plt.ylabel(\"total reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw deterministic policy\n",
    "grid = Gridworld()\n",
    "\n",
    "optimal_Q, opt_policy, total_re = grid.td_learning(2000, 0.03, 0.6, 0.4)\n",
    "\n",
    "values = np.zeros((optimal_Q.shape[0]))\n",
    "for i in range(optimal_Q.shape[0]):\n",
    "    values[i] = max(optimal_Q[i,:])\n",
    "grid.draw_value(values)\n",
    "\n",
    "grid.draw_deterministic_policy(np.array([np.argmax(opt_policy[row,:]) for row in range(grid.state_size)]))\n",
    "\n",
    "print(optimal_Q)\n",
    "print(opt_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw learning curve\n",
    "\n",
    "grid = Gridworld()\n",
    "\n",
    "alpha = [0.05, 0.06, 0.8]\n",
    "\n",
    "alphass = []\n",
    "for alp in alpha:\n",
    "    alphh = []\n",
    "    for i in range(5000):\n",
    "        optimal_Q, opt_policy, total_re, haha = grid.monte_carlo(100, alp, 0.6, 0.4)\n",
    "        alphh.append(total_re)\n",
    "    alphass.append(alphh)\n",
    "    \n",
    "plt.figure()\n",
    "for i in range(len(alphass)):\n",
    "    haha = alphass[i]\n",
    "    rewa00 = np.array(haha)\n",
    "    rewa00.reshape(5000,100)\n",
    "    rewa0 = np.mean(rewa00, axis=0)\n",
    "    plt.plot(range(100), rewa0)\n",
    "plt.xlabel(\"number of episodes, alpha varies\")\n",
    "plt.ylabel(\"total reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw learning curves\n",
    "\n",
    "grid = Gridworld()\n",
    "\n",
    "epsilon = [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "epss = []\n",
    "for eps in epsilon:\n",
    "    epp = []\n",
    "    for i in range(2000):\n",
    "        optimal_Q, opt_policy, total_re, haha = grid.monte_carlo(100, 0.03, eps, 0.4)\n",
    "        epp.append(total_re)\n",
    "    epss.append(epp)\n",
    "    \n",
    "plt.figure()\n",
    "for i in range(len(epss)):\n",
    "    eps0 = epss[i]\n",
    "    rewa00 = np.array(eps0)\n",
    "    rewa00.reshape(2000,100)\n",
    "    rewa0 = np.mean(rewa00, axis=0)\n",
    "    plt.plot(range(100), rewa0)\n",
    "plt.xlabel(\"number of episodes, epsilon varies\")\n",
    "plt.ylabel(\"total reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total reward\n",
    "\n",
    "grid = Gridworld()\n",
    "\n",
    "rewardss=[]\n",
    "for i in range(2000):\n",
    "    optimal_Q, opt_policy, total_re, haha = grid.monte_carlo(100, 0.03, 0.6, 0.4)\n",
    "    rewardss.append(total_re)\n",
    "\n",
    "rewa = np.array(rewardss)\n",
    "rewa.reshape(2000,100)\n",
    "avg_re = np.mean(rewa, axis = 0)\n",
    "standev = np.std(rewa, axis = 0)\n",
    "avg_more = avg_re + standev\n",
    "avg_less = avg_re - standev\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(100), avg_re)\n",
    "plt.plot(range(100),avg_more)\n",
    "plt.plot(range(100), avg_less)\n",
    "plt.xlabel(\"number of episodes\")\n",
    "plt.ylabel(\"total reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Gridworld()\n",
    "optimal_Q, opt_policy, total_re, haha = grid.monte_carlo(2000, 0.03, 0.6, 0.4)\n",
    "\n",
    "values = np.zeros((optimal_Q.shape[0]))\n",
    "for i in range(optimal_Q.shape[0]):\n",
    "    values[i] = max(optimal_Q[i,:])\n",
    "grid.draw_value(values)\n",
    "\n",
    "grid.draw_deterministic_policy(np.array([np.argmax(opt_policy[row,:]) for row in range(grid.state_size)]))\n",
    "\n",
    "print(optimal_Q)\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Gridworld()\n",
    "values, policy, iteration = grid.value_iteration(0.4, 0.00001)\n",
    "policy[:,[0,1,2,3]]= policy[:,[3,0,1,2]]\n",
    "\n",
    "\n",
    "grid.draw_value(values)\n",
    "grid.draw_deterministic_policy(np.array([np.argmax(policy[row,:]) for row in range(grid.state_size)]))\n",
    "print(policy)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
